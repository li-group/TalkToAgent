import os
import json
from dotenv import load_dotenv
from openai import OpenAI

from prompts import get_system_description

from params import get_running_params, get_env_params
from internal_tools import raise_error

running_params = get_running_params()
env, env_params = get_env_params(running_params['system'])

# LLM settings
load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")
client = OpenAI(api_key=api_key)
MODEL = 'gpt-4o'

class Evaluator:
    def __init__(self):
        self.history = []

    def evaluate(self, traj, message):
        """
        Analyze the trajectory generated by error based on the code generated by Coder agent and return suggestions for modification.
        Returns:
            traj (dict): Dictionary of the trajectory, consists of 'x', 'u', and 'r'.
            message (string): Original intention of counterfactual policy generation
        """

        evaluator_prompt = """
        You are a evaluator that evaluates whether the Policy generator agent produced right code for counterfactual policy.
        If you thought that the trajectory from the policy faithfully follow the intention from the message, you can
        confirm the counterfactual policy, otherwise raise an error by calling raise error tool.
        
        For accurate evaluation of the trajectory, here are some descriptions of the control system:
        {system_description}
        
        Also, environment parameters used in process control:
        {env_params}
    
        You will get a great reward if you correctly filter out the wrongly generated counterfactual policy!
        """

        messages = [
            {"role": "system", "content": evaluator_prompt.format(
                system_description=get_system_description(running_params['system']),
                env_params=vars(env)
            )},
            {"role": "user", "content": f"""
            Does the trajectory below faithfully follow the user's intention?
            
            Generated trajectory:
            {traj}
            
            Coordinator message:
            {message}
            """
             }
        ]

        tools = [
            {
                "type": "function",
                "name": "raise_error",
                "description": "Raise an error when the request violates constraints (e.g., asks for MPC, PID).",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "message": {
                            "type": "string",
                            "description": "Error message explaining why the request is not supported."
                        }
                    },
                    "required": ["message"]
                }
            }
        ]

        response = client.chat.completions.create(
            model=MODEL,
            messages=messages,
            functions=tools
        )
        if response.choices[0].message.function_call is not None and response.choices[0].message.function_call.name == 'raise_error':
            error_message = json.loads(response.choices[0].message.function_call.arguments)['message']
            raise_error(error_message)
