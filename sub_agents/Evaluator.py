import json

from prompts import get_system_description
from params import get_running_params, get_env_params, get_LLM_configs
from internal_tools import raise_error

running_params = get_running_params()
env, env_params = get_env_params(running_params['system'])
client, MODEL = get_LLM_configs()

# %% Evaluator agent
class Evaluator:
    def __init__(self):
        self.history = []

    def evaluate(self, traj, message):
        """
        Analyze the trajectory generated by error based on the code generated by Coder agent and return suggestions for modification.
        Args:
            traj (dict): Dictionary of the trajectory, consists of 'x', 'u', and 'r'.
            message (str): Original intention of counterfactual policy generation
        """
        evaluator_prompt = """
        You are a evaluator that evaluates whether the Policy generator agent produced right code for counterfactual policy.
        If you thought that the trajectory from the policy faithfully follow the intention from the message, you can
        confirm the counterfactual policy, otherwise raise an error by calling raise error tool.
        
        For accurate evaluation of the trajectory, here are some descriptions of the control system:
        {system_description}
        
        Also, environment parameters used in process control:
        {env_params}
    
        You will get a great reward if you correctly filter out the wrongly generated counterfactual policy!
        """

        messages = [
            {"role": "system", "content": evaluator_prompt.format(
                system_description=get_system_description(running_params['system']),
                env_params=vars(env)
            )},
            {"role": "user", "content": f"""
            Does the trajectory below faithfully follow the user's intention?
            
            Generated trajectory:
            {traj}
            
            Coordinator message:
            {message}
            """
             }
        ]

        tools = [
            {
                "type": "function",
                "name": "raise_error",
                "description": "Raise an error when the request violates constraints (e.g., asks for MPC, PID).",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "message": {
                            "type": "string",
                            "description": "Error message explaining why the request is not supported."
                        }
                    },
                    "required": ["message"]
                }
            }
        ]

        # We exclusively used 'gpt-4o' for evaluator model for its high capability in reasoning
        MODEL = 'gpt-4o'
        response = client.chat.completions.create(
            model=MODEL,
            messages=messages,
            functions=tools,
        )
        if response.choices[0].message.function_call is not None and response.choices[0].message.function_call.name == 'raise_error':
            error_message = json.loads(response.choices[0].message.function_call.arguments)['message']
            raise_error(error_message)
