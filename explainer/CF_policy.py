import traceback
import numpy as np
from src.pcgym import make_env

from params import running_params, env_params

from sub_agents.Policy_generator import PolicyGenerator
from sub_agents.Evaluator import Evaluator
from sub_agents.Debugger import Debugger

running_params = running_params()
env, env_params = env_params(running_params['system'])

def cf_by_policy(t_begin, t_end, policy, message, team_conversation, max_retries, horizon, return_figure=True, use_debugger=True):
    """
    Counterfactual analysis to future trajectories, according to rule-based policies.
    The policies are generated by successive interactions with coder and evaluator agent.
    i.e.) "What would the trajectory change if I use the bang-bang controller instead of the current RL policy?"
          "Why don't we just use the PID controller instead of the RL policy?"
          "Would you compare the predicted trajectory between our RL policy and bang-bang controller after t-300?"
    """

    begin_index = int(np.round(t_begin / env_params['delta_t']))
    end_index = int(np.round(t_end / env_params['delta_t']))
    len_indices = end_index - begin_index + 1
    horizon += len_indices # Re-adjusting horizon

    env_params['noise'] = False  # For reproducibility
    env = make_env(env_params)

    # Regenerating trajectory data with noise disabled
    evaluator, data = env.get_rollouts({'Actual': policy}, reps=1, get_Q=True)

    generator = PolicyGenerator()
    debugger = Debugger()

    # Generate policy
    CF_policy, code = generator.generate(message, policy)
    print(f"[PolicyGenerator] Initial counterfactual policy generated")
    team_conversation.append({"agent": "PolicyGenerator", "summary": f"Initial policy generated", "full_content": generator.prev_codes[-1]})

    success = False
    trial = 0

    while not success and trial < max_retries:
        try:
            # Running the simulation with counterfactual policy
            cf_settings = {
                'CF_mode': 'policy',
                'begin_index': begin_index,
                'end_index': end_index,
                'CF_policy': CF_policy
            }
            _, data_cf = env.get_rollouts({'New policy': policy}, reps=1, get_Q=False,
                                                  cf_settings=cf_settings)
            data_interval = data_cf['New policy'].copy()
            for k, v in data_interval.items():
                data_interval[k] = v[:, begin_index - 1:begin_index + horizon, :]
            ev = Evaluator()
            ev.evaluate(data_interval, message=message)

            success = True

        except Exception as e:
            trial += 1
            error_message = traceback.format_exc()
            print(f"[Debugger] Error during rollout (trial {trial}):\n{error_message}")
            team_conversation.append({"agent": "Debugger",
                                      "content": f"[Trial {trial}] Error during rollout",
                                      "error_message": error_message
                                      })

            if use_debugger:
                guidance = debugger.debug(code, error_message)
                CF_policy = generator.refine_with_guidance(error_message, guidance)
            else:
                CF_policy = generator.refine_with_error(error_message) # Just use the error message

    log = "[Debugger] Code successfully generated. Rollout complete." if success else "[Debugger] Failed after multiple attempts."
    print(log)
    team_conversation.append(
        {"agent": "PolicyGenerator",
         "content": f"[Trial {trial}] Code successfully generated.",
         "Code": f"{generator.prev_codes[-1]}"
         })

    if success:
        # Append counterfactual results to evaluator object
        evaluator.n_pi += 1
        evaluator.policies['New policy'] = policy
        evaluator.data = data | data_cf

        for al, traj in evaluator.data.items():
            for k, v in traj.items():
                evaluator.data[al][k] = v[:, begin_index - 1:begin_index + horizon, :]
        interval = [begin_index - 1, begin_index + horizon]  # Interval to watch the control results
        figures = [evaluator.plot_data(evaluator.data, interval=interval)]

        # figures = [evaluator.plot_data(evaluator.data)]
        if return_figure:
            return figures
        return evaluator.data